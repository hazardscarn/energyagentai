import json
import os
from pathlib import Path
from google.adk.evaluation import evaluate_agent
from google.adk.agents import Agent

"""
Custom evaluation functions for portfolio manager specific metrics
"""

def evaluate_sql_quality(generated_sql: str, expected_criteria: dict) -> dict:
    """
    Evaluate quality of generated SQL queries
    
    Args:
        generated_sql: The SQL query generated by the agent
        expected_criteria: Expected characteristics of the SQL
        
    Returns:
        dict: Evaluation results with score and feedback
    """
    score = 0.0
    feedback = []
    
    # Check for SELECT * (required for ML models)
    if "SELECT *" in generated_sql.upper():
        score += 0.3
        feedback.append("‚úÖ Uses SELECT * for ML compatibility")
    else:
        feedback.append("‚ùå Should use SELECT * for ML model analysis")
    
    # Check for proper table reference
    if "energyagentai.alberta_energy_ai.customer_base" in generated_sql:
        score += 0.2
        feedback.append("‚úÖ Uses correct table reference")
    else:
        feedback.append("‚ùå Incorrect table reference")
    
    # Check for filters mentioned in criteria
    filters = expected_criteria.get('should_filter_by', [])
    for filter_term in filters:
        if filter_term.lower() in generated_sql.lower():
            score += 0.1
            feedback.append(f"‚úÖ Includes filter: {filter_term}")
        else:
            feedback.append(f"‚ùå Missing filter: {filter_term}")
    
    # Normalize score
    max_score = 0.5 + len(filters) * 0.1
    normalized_score = min(score / max_score, 1.0) if max_score > 0 else 0.0
    
    return {
        'score': normalized_score,
        'feedback': feedback,
        'details': {
            'has_select_star': "SELECT *" in generated_sql.upper(),
            'has_correct_table': "energyagentai.alberta_energy_ai.customer_base" in generated_sql,
            'included_filters': [f for f in filters if f.lower() in generated_sql.lower()]
        }
    }

def evaluate_business_insights(response: str, expected_behavior: dict) -> dict:
    """
    Evaluate quality of business insights in agent response
    
    Args:
        response: Agent's response text
        expected_behavior: Expected characteristics of insights
        
    Returns:
        dict: Evaluation results with score and feedback
    """
    score = 0.0
    feedback = []
    
    # Check for key insight elements
    insight_indicators = [
        'recommendation', 'strategy', 'opportunity', 'factor', 'probability',
        'customers', 'segment', 'increase', 'decrease', 'improve'
    ]
    
    found_indicators = [ind for ind in insight_indicators if ind in response.lower()]
    indicator_score = min(len(found_indicators) / 5, 1.0)  # Up to 5 indicators max score
    score += indicator_score * 0.4
    
    # Check for specific expected elements
    if expected_behavior.get('should_provide_insights'):
        if any(word in response.lower() for word in ['insight', 'finding', 'analysis']):
            score += 0.3
            feedback.append("‚úÖ Provides analytical insights")
        else:
            feedback.append("‚ùå Missing analytical insights")
    
    if expected_behavior.get('should_provide_recommendations'):
        if any(word in response.lower() for word in ['recommend', 'suggest', 'should']):
            score += 0.3
            feedback.append("‚úÖ Provides actionable recommendations")
        else:
            feedback.append("‚ùå Missing actionable recommendations")
    
    return {
        'score': score,
        'feedback': feedback,
        'details': {
            'insight_indicators_found': found_indicators,
            'response_length': len(response),
            'has_quantitative_data': any(char.isdigit() for char in response)
        }
    }


if __name__ == "__main__":
    print("üìä Portfolio Manager Evaluation Suite")
    print("Run with: python -m evaluation.run_evaluation")